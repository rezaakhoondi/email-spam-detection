{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d74692b-6699-4d79-bf98-112cfc515b97",
   "metadata": {},
   "source": [
    "# ðŸ“§ Email Spam Detection with Machine Learning  \n",
    "\n",
    "This project demonstrates how to build a machine learning model to classify emails as **spam** or **ham (not spam)**.  \n",
    "We go through dataset loading, preprocessing, training, evaluation, and interpretation of results step by step.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‘ Table of Contents\n",
    "1. Introduction\n",
    "2. Load and Explore the Dataset\n",
    "3. Data Preprocessing\n",
    "4. Train-Test Split\n",
    "5. Model Training\n",
    "6. Evaluation and Comparison\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b38e32-e0a7-4b11-bfa4-8b5886cf8d06",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "\n",
    "Email spam is one of the most common problems in online communication systems.  \n",
    "It refers to unsolicited or unwanted emails, often sent for advertising, phishing, or spreading malware.  \n",
    "\n",
    "In this project, we build a machine learning model to automatically classify emails into **spam** or **ham (legitimate emails)**.  \n",
    "We will use **text preprocessing**, **feature extraction with TF-IDF**, and a **Naive Bayes classifier** as our baseline model.  \n",
    "The workflow can later be extended with more advanced models such as Logistic Regression, SVM, or ensemble methods.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931cc4a8-4832-4061-877e-688ba67befef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# NLP preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Model selection and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83ca67-7604-439a-b626-b9408f15a04e",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset  \n",
    "\n",
    "We use a publicly available dataset (`spam.csv`) that contains two main columns:  \n",
    "\n",
    "- **v1** â†’ Label (`ham` or `spam`)  \n",
    "- **v2** â†’ Email message text  \n",
    "\n",
    "Our goal is to preprocess the text data and train a model that predicts whether a new email is spam.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c79e60f-125c-47b1-be16-1fd27c12e47c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./spam.csv\")\n",
    "df = df[[\"v1\", \"v2\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a218a07-f184-41a4-b725-f2375e88d9f4",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing  \n",
    "\n",
    "Before training our model, we need to clean and preprocess the text data.  \n",
    "The main preprocessing steps include:  \n",
    "\n",
    "1. **Lowercasing** â†’ Convert all text to lowercase for consistency.  \n",
    "2. **Removing punctuation** â†’ Symbols like `.,!?` do not usually add much value for spam detection.  \n",
    "3. **Tokenization (optional)** â†’ Splitting text into individual words.  \n",
    "4. **Stopword removal (optional)** â†’ Common words like *\"the\", \"is\", \"and\"* can be removed to reduce noise.  \n",
    "5. **TF-IDF transformation** â†’ Convert the cleaned text into numerical feature vectors that machine learning models can understand.  \n",
    "\n",
    "By applying these steps, we ensure that our model focuses on the most relevant parts of the email text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb9d6ab-f944-4e5e-bd89-8cd97a258587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v2'] = df['v2'].str.lower()\n",
    "df['v2'] = df['v2'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+|http\\S+|www\\.\\S+','', str(x)))\n",
    "df['v2'] = df['v2'].apply(lambda x: re.sub(r'\\d+', '', str(x)))\n",
    "df['v2'] = df['v2'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9455e2e-a568-4fbc-81ef-c303bbfd7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v2'] = df['v2'].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3309c205-67db-4f4d-9d39-e328af1ea140",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['v2'] = df['v2'].apply(lambda x:  [w for w in x if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d89f90-f47c-4c4a-8c4d-12c76e596a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['v2'] = df['v2'].apply(lambda x:  [lemmatizer.lemmatize(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a2bb35-2a53-47b0-8032-4d611618c448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['v2']= df['v2'].apply(lambda x: \" \".join(x))  \n",
    "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['v2'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b3bc5-53fc-431e-b85d-0bee0cb4aa05",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split  \n",
    "\n",
    "To evaluate the performance of our model, we split the dataset into:  \n",
    "\n",
    "- **Training set (80%)** â†’ Used to train the model.  \n",
    "- **Testing set (20%)** â†’ Used to evaluate how well the model generalizes to unseen data.  \n",
    "\n",
    "We also apply the same preprocessing steps (like TF-IDF vectorization) to both sets to ensure consistency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a3336a-7390-454f-82c3-9375cc2eb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['v1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d35bc6-84cb-4420-af13-29a34433855a",
   "metadata": {},
   "source": [
    "## 5. Model Training  \n",
    "\n",
    "For our baseline model, we use **Multinomial Naive Bayes** because:  \n",
    "\n",
    "- It is simple and efficient for text classification.  \n",
    "- Works well with word frequency features such as **TF-IDF**.  \n",
    "- Often provides surprisingly strong results compared to more complex models.  \n",
    "\n",
    "Later, we can extend the workflow to include other classifiers (e.g., Logistic Regression, SVM, Random Forest) and compare their performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31767d5a-548b-4db0-9c1e-1b047a11a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b314ac-32b8-49e0-8c7e-40b0c3c4729b",
   "metadata": {},
   "source": [
    "## 6. Evaluation  \n",
    "\n",
    "After training the model, we evaluate it on the **test set** using several metrics:  \n",
    "\n",
    "- **Accuracy** â†’ Overall percentage of correctly classified emails.  \n",
    "- **Precision** â†’ Out of all emails classified as spam, how many were actually spam.  \n",
    "- **Recall (Sensitivity)** â†’ Out of all actual spam emails, how many the model correctly identified.  \n",
    "- **F1-Score** â†’ Harmonic mean of precision and recall, useful for imbalanced datasets.  \n",
    "\n",
    "We also visualize the **confusion matrix** to better understand the modelâ€™s performance in distinguishing between spam and ham emails.  \n",
    "\n",
    "Additionally, plotting the **ROC curve** and computing the **AUC score** can provide insight into the modelâ€™s discrimination ability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51811b0c-c19f-4ec3-852d-9e7d820ad195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9739910313901345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99       965\n",
      "        spam       1.00      0.81      0.89       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.99      0.90      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f3bcaf-9b8f-4619-826e-4dd607ca619f",
   "metadata": {},
   "source": [
    "## 7. Conclusion  \n",
    "\n",
    "- The **Naive Bayes classifier** achieved strong performance in detecting spam emails.  \n",
    "- The model is lightweight, interpretable, and fast to train, making it suitable for real-world spam filtering applications.  \n",
    "- However, there is still room for improvement:  \n",
    "  - Trying more advanced models (Logistic Regression, SVM, Random Forest, or even deep learning).  \n",
    "  - Performing hyperparameter tuning.  \n",
    "  - Using additional text preprocessing techniques (like lemmatization or n-grams).  \n",
    "\n",
    "This project demonstrates a complete **end-to-end workflow** for a text classification problem, from raw data to a working predictive model.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
